---
title: "Airbnb - Edinburgh"
author: "Attila Szuts"
date: "31/01/2021"
output:
  pdf_document:
    toc: true
editor_options:
  chunk_output_type: console
---

# Intro

In this assignment I am going to investigate Airbnb listings from Edinburgh on 27 January, 2020. I chose this city as I have long wanted to visit it, and hopefully after the pandemic I will be able to travel there and perhaps this short analysis could be useful in finding good deals among aibnb listings. I had to use an earlier date for my model as there were quite few listings recently, no wonder. The task was to build a model that can price apartments between 2-6 guests in a given city. 

I am going to build three different models and compare their performance in terms of RMSE and pick the one with the best predictive power on a test set. The first model will be a multiple regression model with hand picked coefficients. The second model will be a multiple regression model using LASSO and finally the third model will be built using random forests approach.

\newpage

# Data prep

The total number of observations before exclusion were 13208. Data cleaning and feature engineering consisted of the following steps/decision points: 

1. Remove $-sign from price related variables (price, cleaning fee, extra people)
2. Property type filtered for only Apartment.
3. Room type converted to factor and factor levels shortened.
4. Cancellation policy converted to factor and super strict category created from `super_strict_30` and `super_strict_60`
5. Bed type and neighbourhood converted to factors.
6. Created variable number of days since first review.
7. Create dummy variables from amenities. Kept only those that were in at least 80 listings. I came up with this number from the histogram of total dataset so that I keep the 100 most frequent amenities.
8. I filtered aparments to only include those that are between 2 and 6 persons.
9. Filtered property types to only include Apartments.
10. Filtered out expensive listings (price > 500 GBP).

All code is available on [github](https://github.com/szutsattila/da3-assignment-1). The total number of observations after filtering was 12221. 

# Descriptives

```{r include=FALSE, include =F, setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=FALSE}
rm(list=ls())
library(tidyverse)
library(skimr)
library(janitor)
library(cowplot)
library(xtable)
library(directlabels)
library(caret)
library(knitr)
library(kableExtra)
library(randomForest)
library(rattle)
library(Hmisc)
source("code/da_helper_functions.R")
df <- read_csv("data/clean/airbnb-edinburgh-workfile.csv") %>% janitor::clean_names() %>%  mutate_if(is.character, factor) 
```

```{r include=FALSE}
df <- df %>% 
  mutate(flag_days_since = ifelse(is.na(n_days_since), 1, 0),
         flag_review_scores_rating = ifelse(is.na(n_review_scores_rating), 1, 0))
```

```{r include=FALSE}
# Business problem

# defining Target variable: price or log_price
df <- df %>% rename(price = gbp_price)

# filter for apartments with 2-6 guests
df <- df %>% 
  filter(n_accommodates >= 2 & n_accommodates <= 6) %>% 
  filter(f_property_type == "Apartment")
```

```{r include=FALSE}
# Descriptives
# summary(df)
# describe(df)
# skim(df)
```


```{r include=FALSE}
###### Price
# log or level?
df %>% ggplot(aes(price)) + geom_histogram(bins = 50) + theme_bw()
df %>% ggplot(aes(price)) + geom_histogram(bins = 50) + scale_x_log10() + theme_bw()

# remove extreme values
df <- df %>% filter(price < 500)
```

```{r include=FALSE}
###### n_accomodates
df %>%
  group_by(n_accommodates) %>%
  summarise(mean_price = mean(price), min_price= min(price), max_price = max(price), n = n())

df %>% 
  ggplot(aes(n_accommodates, price)) +
  geom_point() + 
  geom_smooth(method = "lm", se = F) +
  labs(x="Number of people accomodated",y="Price") +
  theme_bw()

# Density chart 
ggplot(data = df, aes(x=price)) +
  geom_density(aes(color=f_room_type, fill=f_room_type),  na.rm =TRUE, alpha= 0.3) +
  labs(x="Price (GBP)", y="Density", color = "", fill = "") +
  # scale_color_manual(name="",
                     # values=c(color[2],color[1], color[3]),
                     # labels=c("Entire home/apt","Private room", "Shared room")) +
  # scale_fill_manual(name="",
                    # values=c(color[2],color[1], color[3]),
                    # labels=c("Entire home/apt","Private room", "Shared room")) +
  theme_bw()  +
  theme(legend.position = c(0.7,0.7),
        legend.direction = "horizontal",
        legend.background = element_blank(),
        legend.box.background = element_rect(color = "white"))

# Barchart
ggplot(data = df, aes(x = factor(n_accommodates), color = f_room_type, fill = f_room_type)) +
  geom_bar(alpha=0.8, na.rm=T, width = 0.8) +
  # scale_color_manual(name="",
  #                    values=c(color[2],color[1], color[3])) +
  # scale_fill_manual(name="",
  #                   values=c(color[2],color[1],  color[3])) +
  labs(x = "Accomodates (Persons)",y = "Frequency")+
  theme_bw() +
  theme(legend.position = "bottom")
```

```{r include=FALSE}
###### categoricals
categoricals <- c("f_room_type", "f_cancellation_policy", "f_bed_type")

for (i in 1:length(categoricals)) {
  df %>%
    group_by(get(categoricals[i])) %>%
    summarise(mean_price = mean(price) ,  n=n()) %>%
    print
}
```

```{r include=FALSE}
# set infinite values to NA
for (j in 1:ncol(df) ) data.table::set(df, which(is.infinite(df[[j]])), j, NA)
```


```{r include=FALSE}
#How is the average price changing in my district by `room_type` and the `bed_type`?
df %>%
  group_by(f_bed_type, f_room_type) %>%
  dplyr::summarize(mean_price = mean(price, na.rm=TRUE))

df %>%
  group_by(f_bed_type) %>%
  dplyr::summarize(mean_price = mean(price, na.rm=TRUE))

Hmisc::describe(df$price)

# Distribution of price by type below 400
```

Price increases as the number of people accomodated increases, altough the spread increases too.

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=3, fig.height=2}
# Boxplot of price by number of persons accomodated
ggplot(df, aes(x = factor(n_accommodates), y = price)) +
  geom_boxplot(alpha=0.8, na.rm=T, outlier.shape = NA, width = 0.8) +
  stat_boxplot(geom = "errorbar", width = 0.8, size = 0.3, na.rm=T)+
  labs(x = "Accomodates (Persons)",y = "Price (GBP)")+
  scale_y_continuous(expand = c(0.01,0.01), limits=c(0, 400), breaks = seq(0,400, 50))+
  theme_bw()
```

There is not much difference in price between Private and shared rooms, however Entire homes are more expensive than both, again with a much higher variance, too.

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=3, fig.height=2}
# Boxplot of price by room type
ggplot(data = df, aes(x = f_room_type, y = price)) +
  stat_boxplot(aes(group = f_room_type), geom = "errorbar", width = 0.3,
               size = 0.5, na.rm=T)+
  geom_boxplot(aes(group = f_room_type),
               size = 0.5, width = 0.6, alpha = 0.3, na.rm=T, outlier.shape = NA) +
  scale_y_continuous(expand = c(0.01,0.01),limits = c(0,300), breaks = seq(0,300,100)) +
  labs(x = "Room type",y = "Price (GBP)")+
  theme_bw()
```

Interestingly, there is not much difference in price between apartments that have a Bed or just a Couch. Note, that there are very few observations without beds in this dataset.

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=3, fig.height=2}
# boxplot of price by bed type
ggplot(data = df, aes(x = f_bed_type, y = price)) +
  stat_boxplot(aes(group = f_bed_type), geom = "errorbar", width = 0.3,
               size = 0.5, na.rm=T)+
  geom_boxplot(aes(group = f_bed_type),
               size = 0.5, width = 0.6, alpha = 0.3, na.rm=T, outlier.shape = NA) +
  scale_y_continuous(expand = c(0.01,0.01),limits = c(0,300), breaks = seq(0,300,100)) +
  labs(x = "Bed type",y = "Price (GBP)")+
  theme_bw()
```


```{r include=FALSE}
# n_days_since
ggplot(df, aes(n_days_since, price)) + 
  geom_point(alpha=0.1) + 
  geom_smooth(method = "lm")
```




# Regression

In the first model I am going to include coefficients based on the case study that Gabor Bekes and Gabor Kezdi has done. However, I am going to look for interactions in this dataset, instead of using theirs.


```{r include=FALSE}
## Setting up models 
# Basic Variables
basic_lev  <- c("n_accommodates", "n_beds", "f_room_type", "n_days_since", "flag_days_since")

# Factorized variables
basic_add <- c("f_cancellation_policy","f_bed_type")
reviews <- c("n_number_of_reviews","n_review_scores_rating", "flag_review_scores_rating")

#not use p_host_response_rate due to missing obs

# Dummy variables: Extras -> collect all options and create dummies
amenities <-  grep("^am_.*", names(df), value = TRUE)
```


```{r include=FALSE}
## Looking for interactions
#Look up room type interactions
p1 <- price_diff_by_variables2(df, "f_room_type", "am_family_kid_friendly", "Room type", "Family kid friendly")
#Look up canelation policy
p3 <- price_diff_by_variables2(df, "f_cancellation_policy", "am_family_kid_friendly", "Cancellation policy", "Family kid friendly")
p4 <- price_diff_by_variables2(df, "f_cancellation_policy", "am_tv", "Cancellation policy", "TV")

g_interactions <- plot_grid(p1, p3, p4, nrow=3, ncol=1)

X  <- c(paste0("(f_room_type + f_cancellation_policy + f_bed_type) * (",
                paste(amenities, collapse=" + "),")"))
```

As we can see, the interactions used by Gabors are not prevalent in this dataset. This is an interesting external validity finding regarding their models. Even though those were very obvious interaction terms in London, they are not very relevant in Edinborough. However, I am still going to include them in my regression model that is going to use LASSO to find the best possible model to see if maybe there are other interactions that could be useful.

I built three separate models:

1. a baseline model, only using the number of people that the apartment can accommodate as a predictor
2. an extended model with all basic data and amenities.
3. and a complete model with all of the second model's predictors + the interactions.

I created a holdout dataset with 20% of observations to evaluate the models on.

```{r echo=FALSE}
g_interactions
```


```{r include=FALSE}
## Create models & separate holdout set
modellev1 <- " ~ n_accommodates"
modellev2 <- paste0(" ~ ",paste(c(basic_lev,basic_add,reviews,amenities),collapse = " + "))
modellev3 <- paste0(" ~ ",paste(c(basic_lev,basic_add,reviews,amenities,X),collapse = " + "))

# create a holdout set (20% of observations)
smp_size <- floor(0.2 * nrow(df))

# Set the random number generator: It will make results reproducable
set.seed(355)

# create ids:
# 1) seq_len: generate regular sequences
# 2) sample: select random rows from a table
holdout_ids <- sample(seq_len(nrow(df)), size = smp_size)
df$holdout <- 0
df$holdout[holdout_ids] <- 1

#Hold-out set Set
data_holdout <- df %>% filter(holdout == 1)

#Working data set
data_work <- df %>% filter(holdout == 0)
```

Using 5-fold cross validation I checked which one of my three regression models performed best.

```{r include=FALSE, cache=TRUE}
## Cross validation
## N = 5
n_folds=5
# Create the folds
set.seed(355)

folds_i <- sample(rep(1:n_folds, length.out = nrow(data_work) ))
# Create results
model_results_cv <- list()


for (i in (1:3)){
  model_name <-  paste0("modellev",i)
  model_pretty_name <- paste0("(",i,")")

  yvar <- "price"
  xvars <- eval(parse(text = model_name))
  formula <- formula(paste0(yvar,xvars))

  # Initialize values
  rmse_train <- c()
  rmse_test <- c()

  model_work_data <- lm(formula,data = data_work)
  BIC <- BIC(model_work_data)
  nvars <- model_work_data$rank -1
  r2 <- summary(model_work_data)$r.squared

  # Do the k-fold estimation
  for (k in 1:n_folds) {
    test_i <- which(folds_i == k)
    # Train sample: all except test_i
    data_train <- data_work[-test_i, ]
    # Test sample
    data_test <- data_work[test_i, ]
    # Estimation and prediction
    model <- lm(formula,data = data_train)
    prediction_train <- predict(model, newdata = data_train)
    prediction_test <- predict(model, newdata = data_test)

    # Criteria evaluation
    rmse_train[k] <- mse_lev(prediction_train, data_train[,yvar] %>% pull)**(1/2)
    rmse_test[k] <- mse_lev(prediction_test, data_test[,yvar] %>% pull)**(1/2)

  }

  model_results_cv[[model_name]] <- list(yvar=yvar,xvars=xvars,formula=formula,model_work_data=model_work_data,
                                         rmse_train = rmse_train,rmse_test = rmse_test,BIC = BIC,
                                         model_name = model_pretty_name, nvars = nvars, r2 = r2)
}

model <- lm(formula,data = data_train)
prediction_train <- predict(model, newdata = data_train)
prediction_test <- predict(model, newdata = data_test)

t1 <- imap(model_results_cv,  ~{
  as.data.frame(.x[c("rmse_test", "rmse_train")]) %>%
    dplyr::summarise_all(.funs = mean) %>%
    mutate("model_name" = .y , "model_pretty_name" = .x[["model_name"]] ,
           "nvars" = .x[["nvars"]], "r2" = .x[["r2"]], "BIC" = .x[["BIC"]])
}) %>%
  bind_rows()
t1
column_names <- c("Model", "N predictors", "R-squared", "BIC", "Training RMSE",
                 "Test RMSE")

# Nice table produced and saved as .tex without \beign{table}
# -R2, BIC on full work data-n.
# -In sample rmse: average on training data; avg test : average on test data

t1_out <- t1 %>%
  select("model_pretty_name", "nvars", "r2" , "BIC", "rmse_train", "rmse_test")
colnames(t1_out) <- column_names
```

In the table below we can see the three models I have built. We can see that:

1. The second model performs better than the first in terms of variance explained and also in terms of test RMSE. That is, the second model is overall more useful than the first.
2. The second model performs worse than the third in terms of variance explained, however it is much better in terms of test RMSE. Put another way, the third model is overfitting the data.


```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
options(xtable.comment = FALSE)
xtable(t1_out, type = "latex", digits=c(0,0,0,2,0,2,2))
```


```{r include=FALSE}
# # RMSE training vs test graph
# t1_levels <- t1 %>%
#   dplyr::select("nvars", "rmse_train", "rmse_test") %>%
#   gather(var,value, rmse_train:rmse_test) %>%
#   mutate(nvars2=nvars+1) %>%
#   mutate(var = factor(var, levels = c("rmse_train", "rmse_test"),
#                       labels = c("RMSE Training","RMSE Test")))
# 
# ggplot(data = t1_levels,
#                                    aes(x = factor(nvars2), y = value, color=factor(var), group = var)) +
#   geom_line(size=1,show.legend=FALSE, na.rm = TRUE) +
#   scale_y_continuous(name = "RMSE", limits = c(26, 50), breaks = seq(26,50, 2)) +
#   scale_x_discrete( name = "Number of coefficients", expand=c(0.01, 0.01)) +
#   geom_dl(aes(label = var),  method = list("last.points", dl.trans(x=x-1), cex=0.4)) +
#   #scale_colour_discrete(guide = 'none') +
#   theme_bw()
# 
# 

```

\newpage

## Regression with LASSO

When doing regression with LASSO, I started out with the second model, and added all interaction terms to the model.

```{r include=FALSE}

# take model 8 (and find observations where there is no missing data)may
vars_model_2 <- c("price", basic_lev,basic_add,reviews,amenities)
vars_model_3 <- c("price", basic_lev,basic_add,reviews,amenities,X)

# Set lasso tuning parameters
train_control <- trainControl(method = "cv", number = n_folds)
tune_grid <- expand.grid("alpha" = c(1), "lambda" = seq(0.05, 1, by = 0.05))

# # We use model 7 without the interactions so that it is easy to compare later to post lasso ols
# formula <- formula(paste0("price ~ ", paste(setdiff(vars_model_2, "price"), collapse = " + ")))
# 
# set.seed(1234)
# lasso_model <- caret::train(formula,
#                       data = data_work,
#                       method = "glmnet",
#                       preProcess = c("center", "scale"),
#                       trControl = train_control,
#                       tuneGrid = tune_grid,
#                     na.action=na.exclude)
# 
# print(lasso_model$bestTune$lambda)
# 
# lasso_coeffs <- coef(lasso_model$finalModel, lasso_model$bestTune$lambda) %>%
#   as.matrix() %>%
#   as.data.frame() %>%
#   rownames_to_column(var = "variable") %>%
#   rename(coefficient = `1`)  # the column has a name "1", to be renamed
# 
# print(lasso_coeffs)
# 
# lasso_coeffs_nz<-lasso_coeffs %>%
#   filter(coefficient!=0)
# print(nrow(lasso_coeffs_nz))
# 
# # Evaluate model. CV error:
# lasso_cv_rmse <- lasso_model$results %>%
#   filter(lambda == lasso_model$bestTune$lambda) %>%
#   dplyr::select(RMSE)
# print(lasso_cv_rmse[1, 1])

################################################################################

# look at interactions too
formula <- formula(paste0("price ~ ", paste(setdiff(vars_model_3, "price"), collapse = " + ")))

set.seed(1234)
lasso_model <- caret::train(formula,
                      data = data_work,
                      method = "glmnet",
                      preProcess = c("center", "scale"),
                      trControl = train_control,
                      tuneGrid = tune_grid,
                    na.action=na.exclude)

print(lasso_model$bestTune$lambda)

lasso_coeffs <- coef(lasso_model$finalModel, lasso_model$bestTune$lambda) %>%
  as.matrix() %>%
  as.data.frame() %>%
  rownames_to_column(var = "variable") %>%
  rename(coefficient = `1`)  # the column has a name "1", to be renamed

print(lasso_coeffs)

lasso_coeffs_nz<-lasso_coeffs %>%
  filter(coefficient!=0)
print(nrow(lasso_coeffs_nz))

# Evaluate model. CV error:
lasso_cv_rmse <- lasso_model$results %>%
  filter(lambda == lasso_model$bestTune$lambda) %>%
  dplyr::select(RMSE)
print(lasso_cv_rmse[1, 1])
```



## Model Diagnostics

```{r include=FALSE}
model1_level <- model_results_cv[["modellev1"]][["model_work_data"]]
model3_level <- model_results_cv[["modellev3"]][["model_work_data"]]


# look at holdout RMSE - REG
model3_level_work_rmse <- mse_lev(predict(model3_level, newdata = data_work), data_work[,"price"] %>% pull)**(1/2)
model3_level_holdout_rmse <- mse_lev(predict(model3_level, newdata = data_holdout), data_holdout[,"price"] %>% pull)**(1/2)

# look at holdout RMSE - LASSO
lasso_model_work_rmse <- mse_lev(predict(lasso_model, newdata = data_work), data_work[,"price"] %>% pull)**(1/2)
lasso_model_holdout_rmse <- mse_lev(predict(lasso_model, newdata = data_holdout), data_holdout[, "price"] %>% pull)**(1/2)


###################################################
# FIGURES FOR FITTED VS ACTUAL OUTCOME VARIABLES #
###################################################

# Target variable
Ylev <- data_holdout[["price"]]

meanY <-mean(Ylev)
sdY <- sd(Ylev)
meanY_m2SE <- meanY -1.96 * sdY
meanY_p2SE <- meanY + 1.96 * sdY
Y5p <- quantile(Ylev, 0.05, na.rm=TRUE)
Y95p <- quantile(Ylev, 0.95, na.rm=TRUE)

# Predicted values
predictionlev_holdout_pred <- as.data.frame(predict(model3_level, newdata = data_holdout, interval="predict")) %>%
  rename(pred_lwr = lwr, pred_upr = upr)
predictionlev_holdout_conf <- as.data.frame(predict(model3_level, newdata = data_holdout, interval="confidence")) %>%
  rename(conf_lwr = lwr, conf_upr = upr)

predictionlev_holdout <- cbind(data_holdout[,c("price","n_accommodates")],
                               predictionlev_holdout_pred,
                               predictionlev_holdout_conf[,c("conf_lwr","conf_upr")])


# Create data frame with the real and predicted values
d <- data.frame(ylev=Ylev, predlev=predictionlev_holdout[,"fit"] )
# Check the differences
d$elev <- d$ylev - d$predlev
```

Taking a look at Diagnostic data, we can see that our simpler model performed better (`r model3_level_holdout_rmse`) in terms of RMSE than LASSO (`r lasso_model_holdout_rmse`). Our model performed similarly to the model presented in the London Case study, that is, it predicts apartments' prices remarkably well - if they are relatively cheap. It performs quite poorly, however, as price increases. This is most likely due to the fact, that there are quite few expensive apartments in the dataset. 

```{r echo=FALSE, fig.width=3, fig.height=2}
# Plot predicted vs price
ggplot(data = d) +
  geom_point(aes(y=ylev, x=predlev), color = "darkgreen", size = 1,
             shape = 16, alpha = 0.7, show.legend=FALSE, na.rm=TRUE) +
  #geom_smooth(aes(y=ylev, x=predlev), method="lm", color=color[2], se=F, size=0.8, na.rm=T)+
  geom_segment(aes(x = 0, y = 0, xend = 350, yend =350), size=0.5, color="navyblue", linetype=2) +
  coord_cartesian(xlim = c(0, 350), ylim = c(0, 350)) +
  scale_x_continuous(expand = c(0.01,0.01),limits=c(0, 350), breaks=seq(0, 350, by=50)) +
  scale_y_continuous(expand = c(0.01,0.01),limits=c(0, 350), breaks=seq(0, 350, by=50)) +
  labs(y = "Price (GBP)", x = "Predicted Price (GBP)") +
  theme_bw()
```


```{r include=FALSE}
# Redo predicted values at 80% PI
predictionlev_holdout_pred <- as.data.frame(predict(model3_level, newdata = data_holdout, interval="predict", level=0.8)) %>%
  rename(pred_lwr = lwr, pred_upr = upr)
predictionlev_holdout_conf <- as.data.frame(predict(model3_level, newdata = data_holdout, interval="confidence", level=0.8)) %>%
  rename(conf_lwr = lwr, conf_upr = upr)

predictionlev_holdout <- cbind(data_holdout[,c("price","n_accommodates")],
                               predictionlev_holdout_pred,
                               predictionlev_holdout_conf[,c("conf_lwr","conf_upr")])

summary(predictionlev_holdout_pred)

predictionlev_holdout_summary <-
  predictionlev_holdout %>%
  group_by(n_accommodates) %>%
  dplyr::summarise(fit = mean(fit, na.rm=TRUE), pred_lwr = mean(pred_lwr, na.rm=TRUE), pred_upr = mean(pred_upr, na.rm=TRUE),
            conf_lwr = mean(conf_lwr, na.rm=TRUE), conf_upr = mean(conf_upr, na.rm=TRUE))
```


Predicted prices for different sized apartments vary in terms of prediction accuracy. Smaller apartments are priced much worse in comparison to bigger appartments. This is in part because of the linear nature of the model: the same sized prediction interval can mean negative values for cheaper (smaller) appartments. This is an expected outcome with linear models, and there is not much you can do. However, here, the size of the PI is different depending on the number of people accommodated by the appartment.

```{r echo=FALSE, fig.width=3, fig.height=2}
ggplot(predictionlev_holdout_summary, aes(x=factor(n_accommodates))) +
  geom_bar(aes(y = fit ), stat="identity",  fill = "darkgreen", alpha=0.7 ) +
  geom_errorbar(aes(ymin=pred_lwr, ymax=pred_upr, color = "Pred. interval"),width=.2) +
  #geom_errorbar(aes(ymin=conf_lwr, ymax=conf_upr, color = "Conf. interval"),width=.2) +
  scale_y_continuous(name = "Predicted Price (GBP)") +
  scale_x_discrete(name = "Accomodates (Persons)") +
  theme_bw() +
  theme(legend.title= element_blank(),legend.position="none")
```

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
kable(x = predictionlev_holdout_summary, format = "latex", booktabs=TRUE,  digits = 3, row.names = FALSE,
      linesep = "", col.names = c("Accomodates","Prediction","Pred. interval lower",
                                  "Pred. interval upper","Conf.interval lower","Conf.interval upper"))
```

\newpage

# Random forest

As the third model that I am going to build now, I will create a Random Forest using caret. I will use the same dataset to see if there is a meaningful increase in performance to justify the cost of increased running time compared to OLS.

I created three different RF models as well, to choose the best that I can compare with regression models. The first one only incorporated basic variables, the second had the same variables as the second linear model, and the third was the same as the second but with autotuning.

```{r include=FALSE}
# source("code/da_helper_functions.R")
data <- read_csv("data/clean/airbnb-edinburgh-workfile.csv") %>% janitor::clean_names() %>%  mutate_if(is.character, factor) 
data <- data %>% 
  rename(price = gbp_price) %>% 
  mutate(flag_days_since = ifelse(is.na(n_days_since), 1, 0),
         flag_review_scores_rating = ifelse(is.na(n_review_scores_rating), 1, 0)) %>% 
  filter(n_accommodates >= 2 & n_accommodates <= 6) %>% 
  filter(f_property_type == "Apartment") %>% 
  filter(price < 500)
```


```{r include=FALSE, cache=TRUE}
## Createing RF models

# create train and holdout samples -------------------------------------------
# train is where we do it all, incl CV

# set.seed(2801)

# First pick a smaller than usual training set so that models run faster and check if works
# If works, start anew without these two lines
 
# try <- createDataPartition(data$price, p = 0.2, list = FALSE)
# data <- data[try, ]

data_train <- data_work
data_holdout <- data_holdout

dim(data_train)
dim(data_holdout)

# Define models: simpler, extended -----------------------------------------------------------

# Basic Variables inc neighnourhood
basic_vars <- c(
  "n_accommodates", "n_beds", "n_days_since",
  "f_cancellation_policy", "f_bed_type",
  "f_neighbourhood_cleansed")

# reviews
reviews <- c("n_number_of_reviews" ,"n_review_scores_rating", "flag_review_scores_rating") # flag_n_number_of_reviews

# Dummy variables
amenities <-  grep("^am_.*", names(data), value = TRUE)

#interactions for the LASSO
# from ch14
X1  <- c("n_accommodates*f_room_type",  "f_room_type*am_family_kid_friendly",
         "am_air_conditioning*f_room_type", "am_pets_allowed*f_room_type")
# with boroughs
X2  <- c("f_room_type*f_neighbourhood_cleansed",
         "n_accommodates*f_neighbourhood_cleansed" )


predictors_1 <- c(basic_vars)
predictors_2 <- c(basic_vars, reviews, amenities)
predictors_E <- c(basic_vars, reviews, amenities, X1,X2)

# do 5-fold CV
train_control <- trainControl(method = "cv",
                              number = 5,
                              verboseIter = FALSE)


# set tuning
tune_grid <- expand.grid(
  .mtry = c(5, 7, 9),
  .splitrule = "variance",
  .min.node.size = c(5, 10)
)


# simpler model for model A (1)
set.seed(1234)
system.time({
rf_model_1 <- train(
  formula(paste0("price ~", paste0(predictors_1, collapse = " + "))),
  data = data_train,
  method = "ranger",
  trControl = train_control,
  tuneGrid = tune_grid,
  importance = "impurity",
  na.action = na.omit
)
})
rf_model_1

# set tuning for benchamrk model (2)
tune_grid <- expand.grid(
  .mtry = c(8, 10, 12),
  .splitrule = "variance",
  .min.node.size = c(5, 10, 15)
)

set.seed(1234)
system.time({
rf_model_2 <- train(
  formula(paste0("price ~", paste0(predictors_2, collapse = " + "))),
  data = data_train,
  method = "ranger",
  trControl = train_control,
  tuneGrid = tune_grid,
  importance = "impurity",
  na.action = na.omit
)
})

rf_model_2

# auto tuning first
set.seed(1234)
system.time({
  rf_model_2auto <- train(
    formula(paste0("price ~", paste0(predictors_2, collapse = " + "))),
    data = data_train,
    method = "ranger",
    trControl = train_control,
    importance = "impurity",
    na.action = na.omit
  )
})
rf_model_2auto

# evaluate random forests -------------------------------------------------

results <- resamples(
  list(
    model_1  = rf_model_1,
    model_2  = rf_model_2,
    model_2b = rf_model_2auto
    
  )
)
summary(results)

# Save outputs -------------------------------------------------------

# Show Model B rmse shown with all the combinations
rf_tuning_modelB <- rf_model_2$results %>%
  dplyr::select(mtry, min.node.size, RMSE) %>%
  dplyr::rename(nodes = min.node.size) %>%
  spread(key = mtry, value = RMSE)
```


```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
# kable(x = rf_tuning_modelB, format = "latex", digits = 2, caption = "CV RMSE")
```


```{r include=FALSE, cache=TRUE}
# Turning parameter choice 1
result_1 <- matrix(c(
                     rf_model_1$finalModel$mtry,
                     rf_model_2$finalModel$mtry,
                     rf_model_2auto$finalModel$mtry,
                     rf_model_1$finalModel$min.node.size,
                     rf_model_2$finalModel$min.node.size,
                     rf_model_2auto$finalModel$min.node.size

                     ),
                    nrow=3, ncol=2,
                    dimnames = list(c("Model A", "Model B","Model B auto"),
                                    c("Min vars","Min nodes"))
                   )
```



```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
# kable(x = result_1, format = "latex", digits = 2)
```


```{r include=FALSE, cache=TRUE}
# Turning parameter choice 2
result_2 <- matrix(c(mean(results$values$`model_1~RMSE`),
                     mean(results$values$`model_2~RMSE`),
                     mean(results$values$`model_2b~RMSE`)
),
nrow=3, ncol=1,
dimnames = list(c("Model A", "Model B","Model B auto"),
                c(results$metrics[2]))
)
```

In the table below we can see the results of the three different RF model that I've built. Based on RMSE alone, the third model (autotuned) performed the best, however not significantly better.

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
kable(x = result_2, format = "latex", digits = 3) 
```

## Model Diagnostics

```{r include=FALSE, cache=TRUE}
#########################################################################################
# Variable Importance Plots -------------------------------------------------------
#########################################################################################
# first need a function to calculate grouped varimp
group.importance <- function(rf.obj, groups) {
  var.imp <- as.matrix(sapply(groups, function(g) {
    sum(varImp(rf.obj)[g], na.rm = TRUE)
  }))
  colnames(var.imp) <- "MeanDecreaseGini"
  return(var.imp)
}


# variable importance plot
# 1) full varimp plot, full
# 2) varimp plot grouped
# 3) varimp plot , top 10
# 4) varimp plot  w copy, top 10


# rf_model_2_var_imp <- importance(rf_model_2$finalModel)/1000
rf_model_2_var_imp <- varImp(rf_model_2)
rf_model_2_var_imp$importance$varname <- rownames(rf_model_2_var_imp$importance)
rownames(rf_model_2_var_imp$importance) <- NULL
rf_model_2_var_imp_df <-
  data.frame(varname = rf_model_2_var_imp$importance$varname, imp = rf_model_2_var_imp$importance$Overall) %>%
  mutate(varname = gsub("f_neighbourhood_cleansed", "Borough:", varname) ) %>%
  mutate(varname = gsub("f_room_type", "Room type:", varname) ) %>%
  arrange(desc(imp)) %>%
  mutate(imp_percentage = imp/sum(imp))


##############################
# 1) full varimp plot, above a cutoff -> weird?
##############################

# to have a quick look
plot(varImp(rf_model_2))

cutoff = 600
rf_model_2_var_imp_df_cutoff <- rf_model_2_var_imp_df[rf_model_2_var_imp_df$imp>cutoff,]

# ggplot(rf_model_2_var_imp_df_cutoff, aes(x=reorder(varname, imp), y=imp_percentage)) +
#   geom_point(color="darkgreen", size=1) +
#   geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color="darkgreen", size=0.75) +
#   ylab("Importance (Percent)") +
#   xlab("Variable Name") +
#   coord_flip() +
#   scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
#   theme_bw() +
#   theme(axis.text.x = element_text(size=8), axis.text.y = element_text(size=8),
#         axis.title.x = element_text(size=8), axis.title.y = element_text(size=8))



```

On the variable importance plot we can see what are the most frequently used variables to split the trees. The number of people accomodated is the most frequently used, after that the number of beds and then the number of reviews. This gives us an idea what are the most important 'coefficients' in the model.

```{r echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE, fig.height=2}
##############################
# 2) full varimp plot, top 10 only
##############################

# have a version with top 10 vars only
ggplot(rf_model_2_var_imp_df[1:10,], aes(x=reorder(varname, imp), y=imp_percentage)) +
  geom_point(color="darkgreen", size=1) +
  geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color="darkgreen", size=0.75) +
  ylab("Importance (Percent)") +
  xlab("Variable Name") +
  coord_flip() +
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  theme_bw() +
  theme(axis.text.x = element_text(size=8), axis.text.y = element_text(size=8),
        axis.title.x = element_text(size=8), axis.title.y = element_text(size=8))
```


```{r include=FALSE, cache=TRUE}
##############################
# 2) varimp plot grouped 
##############################
# grouped variable importance - keep binaries created off factors together

# varnames <- rf_model_2$finalModel$xNames
# f_neighbourhood_cleansed_varnames <- grep("f_neighbourhood_cleansed",varnames, value = TRUE)
# f_cancellation_policy_varnames <- grep("f_cancellation_policy",varnames, value = TRUE)
# f_bed_type_varnames <- grep("f_bed_type",varnames, value = TRUE)
# f_property_type_varnames <- grep("f_property_type",varnames, value = TRUE)
# f_room_type_varnames <- grep("f_room_type",varnames, value = TRUE)
# 
# groups <- list(f_neighbourhood_cleansed=f_neighbourhood_cleansed_varnames,
#                f_cancellation_policy = f_cancellation_policy_varnames,
#                f_bed_type = f_bed_type_varnames,
#                # f_property_type = f_property_type_varnames,
#                f_room_type = f_room_type_varnames,
#                # f_bathroom = "f_bathroom",
#                n_days_since = "n_days_since",
#                n_accommodates = "n_accommodates",
#                n_beds = "n_beds")
# 
# rf_model_2_var_imp_grouped <- group.importance(rf_model_2$finalModel, groups)
# rf_model_2_var_imp_grouped_df <- data.frame(varname = rownames(rf_model_2_var_imp_grouped),
#                                             imp = rf_model_2_var_imp_grouped[,1])  %>%
#   mutate(imp_percentage = imp/sum(imp))
# 
# ggplot(rf_model_2_var_imp_grouped_df, aes(x=reorder(varname, imp), y=imp_percentage)) +
#   geom_point(color="darkgreen", size=1) +
#   geom_segment(aes(x=varname,xend=varname,y=0,yend=imp_percentage), color="darkgreen", size=0.7) +
#   ylab("Importance (Percent)") +   xlab("Variable Name") +
#   coord_flip() +
#   # expand=c(0,0),
#   scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
# theme_bw() +
#   theme(axis.text.x = element_text(size=8), axis.text.y = element_text(size=8),
#         axis.title.x = element_text(size=8), axis.title.y = element_text(size=8))
# 



#########################################################################################
# Partial Dependence Plots -------------------------------------------------------
#########################################################################################

# TODO
# : somehow adding scale screws up. ideadlly both graphs y beween 70 and 130,
# n:accom should be 1,7 by=1

# FIXME
# should be on holdout, right? pred.grid = distinct_(data_train, "), --> pred.grid = distinct_(data_holdout, )
```

The partial dependence plots shows the average predicted price for different sized appartments. We can use this as a starting point when trying to find an answer to the question of pricing appartmetns.
```{r echo=FALSE, message=FALSE, warning=FALSE, cache=TRUE, fig.width=3, fig.height=2}
pdp_n_acc <- pdp::partial(rf_model_2, pred.var = "n_accommodates", pred.grid = distinct_(data_holdout, "n_accommodates"), train = data_train)
pdp_n_acc %>%
  autoplot( ) +
  geom_point(color="darkgreen", size=2) +
  geom_line(color="darkgreen", size=1) +
  ylab("Predicted price") +
  xlab("Accommodates (persons)") +
  scale_x_continuous(limit=c(2,6), breaks=seq(2,6,1))+
theme_bw()
```


```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
pdp_n_roomtype <- pdp::partial(rf_model_2, pred.var = "f_room_type", pred.grid = distinct_(data_holdout, "f_room_type"), train = data_train)
pdp_n_roomtype %>%
  autoplot( ) +
  geom_point(color="darkgreen", size=2) +
  ylab("Predicted price") +
  xlab("Room type") +
  scale_y_continuous(limits=c(60,120), breaks=seq(60,120, by=10)) +
  theme_bw()


# # Subsample performance: RMSE / mean(y) ---------------------------------------
# # NOTE  we do this on the holdout set.
# 
# # ---- cheaper or more expensive flats - not used in book
# data_holdout_w_prediction <- data_holdout %>%
#   mutate(predicted_price = predict(rf_model_2, newdata = data_holdout))
# 
# 
# 
# ######### create nice summary table of heterogeneity
# a <- data_holdout_w_prediction %>%
#   mutate(is_low_size = ifelse(n_accommodates <= 3, "small apt", "large apt")) %>%
#   group_by(is_low_size) %>%
#   dplyr::summarise(
#     rmse = RMSE(predicted_price, price),
#     mean_price = mean(price),
#     rmse_norm = RMSE(predicted_price, price) / mean(price)
#   )
# 
# 
# b <- data_holdout_w_prediction %>%
#   filter(f_neighbourhood_cleansed %in% c("Westminster", "Camden", "Kensington and Chelsea", "Tower Hamlets", "Hackney", "Newham")) %>%
#   group_by(f_neighbourhood_cleansed) %>%
#   dplyr::summarise(
#     rmse = RMSE(predicted_price, price),
#     mean_price = mean(price),
#     rmse_norm = rmse / mean_price
#   )
# 
# c <- data_holdout_w_prediction %>%
#   filter(f_property_type %in% c("Apartment", "House")) %>%
#   group_by(f_property_type) %>%
#   dplyr::summarise(
#     rmse = RMSE(predicted_price, price),
#     mean_price = mean(price),
#     rmse_norm = rmse / mean_price
#   )
# 
# 
# d <- data_holdout_w_prediction %>%
#   dplyr::summarise(
#     rmse = RMSE(predicted_price, price),
#     mean_price = mean(price),
#     rmse_norm = RMSE(predicted_price, price) / mean(price)
#   )
# 
# # Save output
# colnames(a) <- c("", "RMSE", "Mean price", "RMSE/price")
# colnames(b) <- c("", "RMSE", "Mean price", "RMSE/price")
# colnames(c) <- c("", "RMSE", "Mean price", "RMSE/price")
# d<- cbind("All", d)
# colnames(d) <- c("", "RMSE", "Mean price", "RMSE/price")
# 
# line1 <- c("Type", "", "", "")
# line2 <- c("Apartment size", "", "", "")
# line3 <- c("Borough", "", "", "")
# 
# result_3 <- rbind(line2, a, line1, c, line3, b, d) %>%
#   transform(RMSE = as.numeric(RMSE), `Mean price` = as.numeric(`Mean price`),
#             `RMSE/price` = as.numeric(`RMSE/price`))
# 
# options(knitr.kable.NA = '')
# kable(x = result_3, format = "latex", booktabs=TRUE, linesep = "",digits = c(0,2,1,2), col.names = c("","RMSE","Mean price","RMSE/price")) %>%
#   cat(.,file= paste0(output, "performance_across_subsamples.tex"))
# options(knitr.kable.NA = NULL)
```

# Compare models

And finally we can see the results of the comparisons in the table below. The best model seems to be the simplest Random Forest, based on the holdout RMSE. It is interesting, however, that it performs the worst in the Cross Validation. This suggests that this is the most robust model, because even though it is worse on CV, it's the best in a 'real-world' scenarios, consequently I am going to pick this model as a winner.

It is worth mentioning the case of external validity. There are two questions that should be answered. Are these findings generalizable over time and over space? The first question could be easily answered: just download a different date from this city, run the models and evaluate the results. The same thing could be done for a different city, but I think there would be substantial differences in the model. For example, one important predictor is the neighbourhood, which is different in each city. So it would be interesting to investigate an integration of models to be able to generalize findings across different cities.

```{r compare-models, message=FALSE, warning=FALSE, cache=TRUE, include=FALSE}
# OLS with dummies for area
# using model B

set.seed(1234)
system.time({
ols_model <- train(
  formula(paste0("price", modellev2)),
  data = data_train,
  method = "lm",
  trControl = train_control,
  na.action = na.omit
)
})

ols_model_coeffs <-  ols_model$finalModel$coefficients
ols_model_coeffs_df <- data.frame(
  "variable" = names(ols_model_coeffs),
  "ols_coefficient" = ols_model_coeffs
) %>%
  mutate(variable = gsub("`","",variable))

# LASSO
# using extended model w interactions
vars_model_3 <- vars_model_3[-1]

set.seed(1234)
system.time({
lasso_model <- train(
  formula(paste0("price ~ ", paste(vars_model_3, collapse = " + "))),
  data = data_train,
  method = "glmnet",
  preProcess = c("center", "scale"),
  tuneGrid =  expand.grid("alpha" = 1, "lambda" = seq(0.01, 0.25, by = 0.01)),
  trControl = train_control,
  na.action = na.omit
)
})

lasso_coeffs <- coef(
    lasso_model$finalModel,
    lasso_model$bestTune$lambda) %>%
  as.matrix() %>%
  as.data.frame() %>%
  rownames_to_column(var = "variable") %>%
  rename(lasso_coefficient = `1`)  # the column has a name "1", to be renamed

lasso_coeffs_non_null <- lasso_coeffs[!lasso_coeffs$lasso_coefficient == 0,]

regression_coeffs <- merge(ols_model_coeffs_df, lasso_coeffs_non_null, by = "variable", all=TRUE)
regression_coeffs %>%
   write.csv(file = paste0("out/", "regression_coeffs.csv"))
# 
# # CART
# set.seed(1234)
# system.time({
# cart_model <- train(
#   formula(paste0("price ~", paste0(predictors_2, collapse = " + "))),
#   data = data_train,
#   method = "rpart",
#   tuneLength = 10,
#   trControl = train_control,
#   na.action = na.omit
# )
# })
```


```{r eval=FALSE, cache=TRUE, include=FALSE}
fancyRpartPlot(cart_model$finalModel, sub = "")
```


```{r include=FALSE, cache=TRUE}
# GBM  -------------------------------------------------------
# gbm_grid <-  expand.grid(interaction.depth = c(1, 5, 10), # complexity of the tree
#                          n.trees = (4:10)*50, # number of iterations, i.e. trees
#                          shrinkage = 0.1, # learning rate: how quickly the algorithm adapts
#                          n.minobsinnode = 20 # the minimum number of training set samples in a node to commence splitting
# )
# 
# 
# set.seed(1234)
# system.time({
#   gbm_model <- train(formula(paste0("price ~", paste0(predictors_2, collapse = " + "))),
#                      data = data_train,
#                      method = "gbm",
#                      trControl = train_control,
#                      verbose = FALSE,
#                      tuneGrid = gbm_grid,
#                      na.action = na.omit)
# })
# gbm_model
# 

# much more tuning

#  faster, for testing
#gbm_grid2 <-  expand.grid(interaction.depth = c( 5, 7, 9, 11), # complexity of the tree
#                          n.trees = (1:10)*50, # number of iterations, i.e. trees
#                          shrinkage = c(0.05, 0.1), # learning rate: how quickly the algorithm adapts
#                          n.minobsinnode = c(10,20) # the minimum number of training set samples in a node to commence splitting
#)


# the next will be in final model, loads of tuning
# gbm_grid2 <-  expand.grid(interaction.depth = c(1, 3, 5, 7, 9, 11), # complexity of the tree
#                            n.trees = (1:10)*50, # number of iterations, i.e. trees
#                            shrinkage = c(0.02, 0.05, 0.1, 0.15, 0.2), # learning rate: how quickly the algorithm adapts
#                            n.minobsinnode = c(5,10,20,30) # the minimum number of training set samples in a node to commence splitting
# )
# 
# 
# set.seed(1234)
# system.time({
#   gbm_model2 <- train(formula(paste0("price ~", paste0(predictors_2, collapse = " + "))),
#                       data = data_train,
#                       method = "gbm",
#                       trControl = train_control,
#                       verbose = FALSE,
#                       tuneGrid = gbm_grid2,
#                       na.action = na.omit)
# })
# gbm_model2


# and get prediction rmse and add to next summary table

# ---- compare these models

final_models <-
  list("OLS" = ols_model,
  "LASSO (model w/ interactions)" = lasso_model,
  # "CART" = cart_model,
  "Random forest (smaller model)" = rf_model_1,
  "Random forest" = rf_model_2,
  "Random forest (auto tuned)" = rf_model_2auto)
  # "GBM (basic tuning)"  = gbm_model,
  # "GBM (broad tuning)" = gbm_model2)

results <- resamples(final_models) %>% summary()


# Save output --------------------------------------------------------
# Model selection is carried out on this CV RMSE

result_4 <- imap(final_models, ~{
  mean(results$values[[paste0(.y,"~RMSE")]])
}) %>% unlist() %>% as.data.frame() %>%
  rename("CV RMSE" = ".")
```


```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
# kable(x = result_4, format = "latex", digits = 3, booktabs=TRUE, linesep = "")
```


```{r include=FALSE, cache=TRUE}
# evaluate preferred model on the holdout set -----------------------------

result_5 <- map(final_models, ~{
  RMSE(predict(.x, newdata = data_holdout), data_holdout[["price"]])
}) %>% unlist() %>% as.data.frame() %>%
  rename("Holdout RMSE" = ".")

```

Note: I wasn't able to figure out the reason why the table prints out a different Holdout RMSE, it should be: `r round(lasso_model_holdout_rmse, 3)`.

```{r echo=FALSE, message=FALSE, warning=FALSE, results='asis'}
kable(x = cbind(result_4, result_5), format = "latex", digits = 3, booktabs=TRUE, linesep = "")

```
